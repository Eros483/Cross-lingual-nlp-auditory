# ğŸ§ Multi-Lingual Audio-Based Sentiment Detection

**Languages Supported:** English, French, German, Italian, EspaÃ±ol  
**Modalities:** Audio (MFCC via CNN) + Text (XLM-RoBERTa) + Fusion (MLP on logits)

---

## ğŸ—‚ï¸ Directory Structure

```
AUDIO-NLP/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â””â”€â”€ models/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ audio_cnn.py
â”‚   â”‚       â”œâ”€â”€ fusion_mlp_model.py
â”‚   â”œâ”€â”€ pytorch_states/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ audio_model.py
â”‚   â”œâ”€â”€ fusion_model.py
â”‚   â”œâ”€â”€ text_model.py
â”‚   â”œâ”€â”€ whisper_utils.py
â”‚   â”œâ”€â”€ wheels/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ download_models.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ fusion/
â”‚   â”œâ”€â”€ processed_audio/
â”‚   â”œâ”€â”€ processed_text/
â”‚   â”œâ”€â”€ ravdess/
â”‚   â”œâ”€â”€ tess_audio/
â”‚   â”œâ”€â”€ ravdess_dataset_download.ipynb
â”‚   â””â”€â”€ textualData.csv
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ data_preparation.ipynb
â”‚   â”œâ”€â”€ train_audio_cnn.ipynb
â”‚   â”œâ”€â”€ train_fusion.ipynb
â”‚   â””â”€â”€ train_text_emotion.ipynb
â”œâ”€â”€ utils/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## âœ… Stepwise Plan

### 1. Dataset Preparation
- **TESS**: Preprocess audio clips to MFCC â†’ `processed_audio/`
- **Kaggle Multilingual Dataset**: Clean multilingual text â†’ `processed_text/`
- Align emotion labels for audio/text fusion  
- Create validation splits

### 2. Audio Emotion Model
- Train CNN on MFCC features  
- Notebook: `train_audio_cnn.ipynb`  
- Save model as `audio_cnn.pt`

### 3. Text Emotion Model
- Fine-tune XLM-RoBERTa  
- Notebook: `train_text_emotion.ipynb`  
- Save model as `text_emotion_model.pt`

### 4. Fusion Model
- Use predictions from audio and text models  
- Train MLP on combined outputs  
- Notebook: `train_fusion.ipynb`  
- Save as `fusion_model.pkl`

### 5. Backend Setup
- Whisper â†’ transcript
- Run XLM-R for text emotion
- Run CNN for audio emotion
- Combine using fusion model
- FastAPI endpoint: `/analyze/`

### 6. Deployment
- Dockerized backend ready (see `Dockerfile`)
- GPU optimized
- Frontend optional (basic Streamlit prototype exists)

---

## âš ï¸ Notes
- Emotion classes across datasets are aligned (manually).
- No joint audio-text data â†’ fusion is trained on separate dataset predictions.
- Consider collecting paired audio+text later for supervised multimodal learning.
