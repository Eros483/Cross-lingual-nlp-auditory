{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5353c2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d3a7a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=np.load(\"../datasets/processed_audio/features.npy\", allow_pickle=True)\n",
    "labels=np.load(\"../datasets/processed_audio/labels.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39df0bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val=train_test_split(features, labels, test_size=0.2, random_state=42, stratify=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3a2ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x= torch.tensor(self.features[idx], dtype=torch.float32).unsqueeze(0)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "train_dataset = AudioDataset(X_train, y_train)\n",
    "val_dataset = AudioDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38869ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3, 3), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3, 3), padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * (X_train.shape[1]//4) * (X_train.shape[2]//4), 128)  # Adjust based on input size\n",
    "        self.fc2 = nn.Linear(128, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x=self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x=torch.flatten(x, 1)\n",
    "        x=self.dropout(self.relu(self.fc1(x)))\n",
    "        x=self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model=AudioCNN(n_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f46184d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(691, 200, 40)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6b66dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arnab\\miniconda3\\envs\\emotion-detection\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24901a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output=model(x)\n",
    "\n",
    "        loss = criterion(output, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss.item()*x.size(0)\n",
    "        preds=output.argmax(dim=1)\n",
    "        correct+=(preds==y).sum().item()\n",
    "    return total_loss/len(loader.dataset),correct/len(loader.dataset)\n",
    "\n",
    "def eval_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            output=model(x)\n",
    "\n",
    "            loss = criterion(output, y)\n",
    "            total_loss+=loss.item()*x.size(0)\n",
    "            preds=output.argmax(dim=1)\n",
    "            correct+=(preds==y).sum().item()\n",
    "    \n",
    "    return total_loss/len(loader.dataset),correct/len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16f99b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 2.0408, Train Acc: 0.2663, Val Loss: 1.5399, Val Acc: 0.2717\n",
      "Epoch 2/20, Train Loss: 1.4537, Train Acc: 0.3343, Val Loss: 1.2835, Val Acc: 0.4393\n",
      "Epoch 3/20, Train Loss: 1.3170, Train Acc: 0.4327, Val Loss: 1.2067, Val Acc: 0.4913\n",
      "Epoch 4/20, Train Loss: 1.2034, Train Acc: 0.5137, Val Loss: 1.1643, Val Acc: 0.5491\n",
      "Epoch 5/20, Train Loss: 1.0969, Train Acc: 0.5687, Val Loss: 1.1620, Val Acc: 0.4624\n",
      "Epoch 6/20, Train Loss: 1.0556, Train Acc: 0.5601, Val Loss: 1.0227, Val Acc: 0.5665\n",
      "Epoch 7/20, Train Loss: 0.9037, Train Acc: 0.6469, Val Loss: 0.9668, Val Acc: 0.5954\n",
      "Epoch 8/20, Train Loss: 0.7979, Train Acc: 0.6643, Val Loss: 0.9304, Val Acc: 0.6474\n",
      "Epoch 9/20, Train Loss: 0.7423, Train Acc: 0.7192, Val Loss: 0.8628, Val Acc: 0.6301\n",
      "Epoch 10/20, Train Loss: 0.6164, Train Acc: 0.7598, Val Loss: 0.8599, Val Acc: 0.6474\n",
      "Epoch 11/20, Train Loss: 0.5890, Train Acc: 0.7742, Val Loss: 0.7764, Val Acc: 0.6821\n",
      "Epoch 12/20, Train Loss: 0.4989, Train Acc: 0.8046, Val Loss: 0.7643, Val Acc: 0.6936\n",
      "Epoch 13/20, Train Loss: 0.3657, Train Acc: 0.8741, Val Loss: 0.7421, Val Acc: 0.7168\n",
      "Epoch 14/20, Train Loss: 0.3531, Train Acc: 0.8741, Val Loss: 0.6895, Val Acc: 0.7052\n",
      "Epoch 15/20, Train Loss: 0.3434, Train Acc: 0.8741, Val Loss: 0.9540, Val Acc: 0.6705\n",
      "Epoch 16/20, Train Loss: 0.3717, Train Acc: 0.8466, Val Loss: 0.7870, Val Acc: 0.6936\n",
      "Epoch 17/20, Train Loss: 0.2839, Train Acc: 0.8929, Val Loss: 0.8429, Val Acc: 0.6994\n",
      "Epoch 18/20, Train Loss: 0.2186, Train Acc: 0.9219, Val Loss: 0.7207, Val Acc: 0.7341\n",
      "Epoch 19/20, Train Loss: 0.1839, Train Acc: 0.9450, Val Loss: 0.7567, Val Acc: 0.7457\n",
      "Epoch 20/20, Train Loss: 0.2107, Train Acc: 0.9334, Val Loss: 0.8196, Val Acc: 0.7341\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc= train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc= eval_epoch(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6fbf10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({4: 192, 1: 192, 0: 192, 3: 192, 2: 96})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a6c69a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../training/models/audio_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "839f9c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=AudioCNN(n_classes=5)\n",
    "model.load_state_dict(torch.load(\"../training/models/audio_cnn.pt\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "all_audio_probs=[]\n",
    "all_labels=[]\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in val_loader:\n",
    "        x = x.to(device)\n",
    "        output = model(x)\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        all_audio_probs.append(probs.cpu())\n",
    "        all_labels.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97df2649",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_val_probs = torch.cat(all_audio_probs, dim=0)\n",
    "fusion_val_labels= torch.cat(all_labels, dim=0)\n",
    "audio_preds = audio_val_probs.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7811a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: audio_val_probs.pt and fusion_val_labels.pt\n"
     ]
    }
   ],
   "source": [
    "torch.save(audio_val_probs, \"../datasets/fusion/audio_val_probs.pt\")\n",
    "torch.save(fusion_val_labels, \"../datasets/fusion/fusion_val_labels.pt\")\n",
    "torch.save(audio_preds, \"../datasets/fusion/audio_val_preds.pt\")\n",
    "\n",
    "print(\"Saved: audio_val_probs.pt and fusion_val_labels.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
